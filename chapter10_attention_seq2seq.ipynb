{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noble-mexican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc4\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naval-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing data\n",
    "# 2. build model\n",
    "# 2.1 encoder\n",
    "# 2.2 attention\n",
    "# 2.3 decoder\n",
    "# 3. evaluation\n",
    "# 3.1 given sentence, return translated results\n",
    "# 3.2 visualize results (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becoming-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why me?\n",
      "¿Por que yo?\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = './data_spa_en/spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "en_sentence = 'Why me?'\n",
    "spa_sentence = '¿Por qué yo?'\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(spa_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identified-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> why me ? <end>\n",
      "<start> ¿ por que yo ? <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([?.!,¿])\",r\" \\1 \",s)\n",
    "    s = re.sub(r'[\" \"]+',\" \",s)\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \",s)\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(spa_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "loving-invite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> go . <end>', '<start> ve . <end>')\n",
      "118964\n",
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    print(preprocessed_sentence_pairs[0])\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "strange-montgomery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 135   3   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "16\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, filters='',split=' '\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return min(len(t) for t in tensor)\n",
    "\n",
    "print(input_tensor[0])\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "\n",
    "print(max_length_input)\n",
    "print(max_length_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "balanced-thanks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
    "\n",
    "len(input_train), len(input_eval), len(output_train), len(output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "statistical-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> <start>\n",
      "37 --> tengo\n",
      "11 --> que\n",
      "7099 --> pintarlo\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "\n",
      "1 --> <start>\n",
      "4 --> i\n",
      "29 --> have\n",
      "15 --> to\n",
      "1343 --> paint\n",
      "10 --> it\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d --> %s' % (t, tokenizer.index_word[t]))\n",
    "\n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "contained-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "medieval-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[   1  451  229 ...    0    0    0]\n",
      " [   1   21 3802 ...    0    0    0]\n",
      " [   1    7   21 ...    0    0    0]\n",
      " ...\n",
      " [   1   22   33 ...    0    0    0]\n",
      " [   1   12  132 ...    0    0    0]\n",
      " [   1    8   35 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   1    4  290    4   92  326    3    2    0    0    0]\n",
      " [   1    9 2522   49   56    3    2    0    0    0    0]\n",
      " [   1   10    8    9 1714    3    2    0    0    0    0]\n",
      " [   1   25    6   36  898    7    2    0    0    0    0]\n",
      " [   1   20   11   70   79  828    3    2    0    0    0]\n",
      " [   1    6   88  374    3    2    0    0    0    0    0]\n",
      " [   1   24   28   39    7    2    0    0    0    0    0]\n",
      " [   1    4   75   40  715  284    3    2    0    0    0]\n",
      " [   1   91  170  144    5    3    2    0    0    0    0]\n",
      " [   1   82   25   12   14   53    7    2    0    0    0]\n",
      " [   1    4   18  209  261   21  134    3    2    0    0]\n",
      " [   1    4   30   12  127 1578    3    2    0    0    0]\n",
      " [   1    4   65  105   21  505    3    2    0    0    0]\n",
      " [   1  111   19    3    2    0    0    0    0    0    0]\n",
      " [   1    6   87   12  116   17    3    2    0    0    0]\n",
      " [   1   20   11 1301   49  107    7    2    0    0    0]\n",
      " [   1   64   31  312    3    2    0    0    0    0    0]\n",
      " [   1    4  114   10   11  197    3    2    0    0    0]\n",
      " [   1    5   51   32   14  270    3    2    0    0    0]\n",
      " [   1    5 1210    3    2    0    0    0    0    0    0]\n",
      " [   1    4   77   13  827    3    2    0    0    0    0]\n",
      " [   1   16   38  166   54   10    3    2    0    0    0]\n",
      " [   1   46    5  120    3    2    0    0    0    0    0]\n",
      " [   1   20   11   61  616    3    2    0    0    0    0]\n",
      " [   1   16   92 3137    3    2    0    0    0    0    0]\n",
      " [   1 1272    8   67    4   47    3    2    0    0    0]\n",
      " [   1    4  291   33 1588    3    2    0    0    0    0]\n",
      " [   1    6   65  160 1120    3    2    0    0    0    0]\n",
      " [   1   14  548   15   21  449    3    2    0    0    0]\n",
      " [   1    4   29  105   21 1264    3    2    0    0    0]\n",
      " [   1   10   38   93   54  342    3    2    0    0    0]\n",
      " [   1    5    8   45   11 4897    3    2    0    0    0]\n",
      " [   1    4   18   67 1030   74    3    2    0    0    0]\n",
      " [   1    4   38 1043   10    3    2    0    0    0    0]\n",
      " [   1   46   17 2893   10    3    2    0    0    0    0]\n",
      " [   1   10   11  183  448    3    2    0    0    0    0]\n",
      " [   1  659   15   13 4028    3    2    0    0    0    0]\n",
      " [   1    5  180   15  776   45    3    2    0    0    0]\n",
      " [   1   14   11 1968    3    2    0    0    0    0    0]\n",
      " [   1  516   10  128    3    2    0    0    0    0    0]\n",
      " [   1    6   23   34   84   59   81    3    2    0    0]\n",
      " [   1  396   11   44   17    3    2    0    0    0    0]\n",
      " [   1   60  117   13  788    7    2    0    0    0    0]\n",
      " [   1  775    3    2    0    0    0    0    0    0    0]\n",
      " [   1   27 4177 2201    3    2    0    0    0    0    0]\n",
      " [   1   53   50   39    3    2    0    0    0    0    0]\n",
      " [   1  171   68   68  243    3    2    0    0    0    0]\n",
      " [   1   14   26 1168    3    2    0    0    0    0    0]\n",
      " [   1  651   34   58   49  181    7    2    0    0    0]\n",
      " [   1    4  169  302    6    3    2    0    0    0    0]\n",
      " [   1   46   17   36   37    2    0    0    0    0    0]\n",
      " [   1  132   49   30   12  900   37    2    0    0    0]\n",
      " [   1    6   63   10    3    2    0    0    0    0    0]\n",
      " [   1   66   49    4   87   12   73    3    2    0    0]\n",
      " [   1   16   23 2256    3    2    0    0    0    0    0]\n",
      " [   1  271    6   22   20    7    2    0    0    0    0]\n",
      " [   1    4   63  162   73    3    2    0    0    0    0]\n",
      " [   1   52   22   16   22   20    7    2    0    0    0]\n",
      " [   1   57    8   66  239    3    2    0    0    0    0]\n",
      " [   1    4 1670  119    5    3    2    0    0    0    0]\n",
      " [   1  181   22   16  264    7    2    0    0    0    0]\n",
      " [   1   20   26    9  288  122    3    2    0    0    0]\n",
      " [   1    6   65   76   17    3    2    0    0    0    0]\n",
      " [   1    4   18   34    9  463    3    2    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "green-commerce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9414\n"
     ]
    }
   ],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "governing-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 16, 256)\n",
      "sample_output.shape:  (64, 16, 1024)\n",
      "sample_hidden.shpae:  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, \n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shpae: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-outside",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
