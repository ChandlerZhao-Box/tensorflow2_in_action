{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agricultural-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc4\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing data\n",
    "# 2. build model\n",
    "# 2.1 encoder\n",
    "# 2.2 attention\n",
    "# 2.3 decoder\n",
    "# 3. evaluation\n",
    "# 3.1 given sentence, return translated results\n",
    "# 3.2 visualize results (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lesser-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why me?\n",
      "¿Por que yo?\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = './data_spa_en/spa.txt'\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "en_sentence = 'Why me?'\n",
    "spa_sentence = '¿Por qué yo?'\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(spa_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sitting-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> why me ? <end>\n",
      "<start> ¿ por que yo ? <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([?.!,¿])\",r\" \\1 \",s)\n",
    "    s = re.sub(r'[\" \"]+',\" \",s)\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \",s)\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(spa_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "diverse-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> go . <end>', '<start> ve . <end>')\n",
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n",
      "118964\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    print(preprocessed_sentence_pairs[0])\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])\n",
    "print(len(en_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "orange-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 135   3   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "16\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, filters='',split=' '\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return min(len(t) for t in tensor)\n",
    "\n",
    "print(input_tensor[0])\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "\n",
    "print(max_length_input)\n",
    "print(max_length_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "colonial-constitutional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
    "\n",
    "len(input_train), len(input_eval), len(output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unlikely-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> <start>\n",
      "37 --> tengo\n",
      "11 --> que\n",
      "7099 --> pintarlo\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "\n",
      "1 --> <start>\n",
      "4 --> i\n",
      "29 --> have\n",
      "15 --> to\n",
      "1343 --> paint\n",
      "10 --> it\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d --> %s' % (t, tokenizer.index_word[t]))\n",
    "\n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "apart-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "celtic-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[   1  451  229 ...    0    0    0]\n",
      " [   1   21 3802 ...    0    0    0]\n",
      " [   1    7   21 ...    0    0    0]\n",
      " ...\n",
      " [   1   22   33 ...    0    0    0]\n",
      " [   1   12  132 ...    0    0    0]\n",
      " [   1    8   35 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   1    4  290    4   92  326    3    2    0    0    0]\n",
      " [   1    9 2522   49   56    3    2    0    0    0    0]\n",
      " [   1   10    8    9 1714    3    2    0    0    0    0]\n",
      " [   1   25    6   36  898    7    2    0    0    0    0]\n",
      " [   1   20   11   70   79  828    3    2    0    0    0]\n",
      " [   1    6   88  374    3    2    0    0    0    0    0]\n",
      " [   1   24   28   39    7    2    0    0    0    0    0]\n",
      " [   1    4   75   40  715  284    3    2    0    0    0]\n",
      " [   1   91  170  144    5    3    2    0    0    0    0]\n",
      " [   1   82   25   12   14   53    7    2    0    0    0]\n",
      " [   1    4   18  209  261   21  134    3    2    0    0]\n",
      " [   1    4   30   12  127 1578    3    2    0    0    0]\n",
      " [   1    4   65  105   21  505    3    2    0    0    0]\n",
      " [   1  111   19    3    2    0    0    0    0    0    0]\n",
      " [   1    6   87   12  116   17    3    2    0    0    0]\n",
      " [   1   20   11 1301   49  107    7    2    0    0    0]\n",
      " [   1   64   31  312    3    2    0    0    0    0    0]\n",
      " [   1    4  114   10   11  197    3    2    0    0    0]\n",
      " [   1    5   51   32   14  270    3    2    0    0    0]\n",
      " [   1    5 1210    3    2    0    0    0    0    0    0]\n",
      " [   1    4   77   13  827    3    2    0    0    0    0]\n",
      " [   1   16   38  166   54   10    3    2    0    0    0]\n",
      " [   1   46    5  120    3    2    0    0    0    0    0]\n",
      " [   1   20   11   61  616    3    2    0    0    0    0]\n",
      " [   1   16   92 3137    3    2    0    0    0    0    0]\n",
      " [   1 1272    8   67    4   47    3    2    0    0    0]\n",
      " [   1    4  291   33 1588    3    2    0    0    0    0]\n",
      " [   1    6   65  160 1120    3    2    0    0    0    0]\n",
      " [   1   14  548   15   21  449    3    2    0    0    0]\n",
      " [   1    4   29  105   21 1264    3    2    0    0    0]\n",
      " [   1   10   38   93   54  342    3    2    0    0    0]\n",
      " [   1    5    8   45   11 4897    3    2    0    0    0]\n",
      " [   1    4   18   67 1030   74    3    2    0    0    0]\n",
      " [   1    4   38 1043   10    3    2    0    0    0    0]\n",
      " [   1   46   17 2893   10    3    2    0    0    0    0]\n",
      " [   1   10   11  183  448    3    2    0    0    0    0]\n",
      " [   1  659   15   13 4028    3    2    0    0    0    0]\n",
      " [   1    5  180   15  776   45    3    2    0    0    0]\n",
      " [   1   14   11 1968    3    2    0    0    0    0    0]\n",
      " [   1  516   10  128    3    2    0    0    0    0    0]\n",
      " [   1    6   23   34   84   59   81    3    2    0    0]\n",
      " [   1  396   11   44   17    3    2    0    0    0    0]\n",
      " [   1   60  117   13  788    7    2    0    0    0    0]\n",
      " [   1  775    3    2    0    0    0    0    0    0    0]\n",
      " [   1   27 4177 2201    3    2    0    0    0    0    0]\n",
      " [   1   53   50   39    3    2    0    0    0    0    0]\n",
      " [   1  171   68   68  243    3    2    0    0    0    0]\n",
      " [   1   14   26 1168    3    2    0    0    0    0    0]\n",
      " [   1  651   34   58   49  181    7    2    0    0    0]\n",
      " [   1    4  169  302    6    3    2    0    0    0    0]\n",
      " [   1   46   17   36   37    2    0    0    0    0    0]\n",
      " [   1  132   49   30   12  900   37    2    0    0    0]\n",
      " [   1    6   63   10    3    2    0    0    0    0    0]\n",
      " [   1   66   49    4   87   12   73    3    2    0    0]\n",
      " [   1   16   23 2256    3    2    0    0    0    0    0]\n",
      " [   1  271    6   22   20    7    2    0    0    0    0]\n",
      " [   1    4   63  162   73    3    2    0    0    0    0]\n",
      " [   1   52   22   16   22   20    7    2    0    0    0]\n",
      " [   1   57    8   66  239    3    2    0    0    0    0]\n",
      " [   1    4 1670  119    5    3    2    0    0    0    0]\n",
      " [   1  181   22   16  264    7    2    0    0    0    0]\n",
      " [   1   20   26    9  288  122    3    2    0    0    0]\n",
      " [   1    6   65   76   17    3    2    0    0    0    0]\n",
      " [   1    4   18   34    9  463    3    2    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "southeast-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9414\n"
     ]
    }
   ],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "descending-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 16, 256)\n",
      "sample_output.shape:  (64, 16, 1024)\n",
      "sample_hidden.shpae:  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, \n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shpae: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "mineral-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_results.shape:  (64, 1024)\n",
      "attention_weights.shape:  (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        #decoder_hidden.shape: (batch_size, units)\n",
    "        #encoder_outputs.shape: (batch_size, length, units)\n",
    "#         print(decoder_hidden.shape)\n",
    "#         print(encoder_outputs.shape)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "#         print(decoder_hidden_with_time_axis.shape)\n",
    "#         print(self.W1(encoder_outputs).shape)\n",
    "#         print(self.W2(decoder_hidden_with_time_axis).shape)\n",
    "#         print((self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)).shape)\n",
    "        #before V: (batch_size, length, units)\n",
    "        #after V: (batch_size, length, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "#         print(score.shape)\n",
    "        #shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "#         print(context_vector.shape)\n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "#         print(context_vector.shape)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "attention_model = BahdanauAttention(units=10)\n",
    "context_vector, attention_weights = attention_model(sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention_results.shape: \", context_vector.shape)\n",
    "print(\"attention_weights.shape: \", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "voluntary-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(1, 3)\n",
      "tf.Tensor(\n",
      "[[2 3 4]\n",
      " [5 6 7]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.constant([[1,2,3],[4,5,6]])\n",
    "t2 = tf.constant([[1,1,1]])\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "\n",
    "print(t1+t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cutting-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4935)\n",
      "(64, 1024)\n",
      "(64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "        \n",
    "    # x decoder当前步的输入\n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n",
    "        \n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        # after embedding: x.shape: (batch_size, 1, embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        combined_x = tf.concat([tf.expand_dims(context_vector, 1),x], axis = -1)\n",
    "        \n",
    "        #output.shape: [batch_size, 1, decoding_units]\n",
    "        #state.shape: [batch_size, decoding_units]\n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        #output.shape: [batch_size, decoding_units]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "decoder_output, decoder_hidden, attention_weights = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                                          sample_hidden,\n",
    "                                          sample_output)\n",
    "\n",
    "print(decoder_output.shape)\n",
    "print(decoder_hidden.shape)\n",
    "print(attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "thorough-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction='none')\n",
    "def loss_function(real, predict):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, predict)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "negative-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, target, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(input, encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # <start> I am here <end>\n",
    "        #1. <start> -> I\n",
    "        #2. I -> am\n",
    "        #3. am -> here\n",
    "        #4. here -> <end>\n",
    "        for t in range(0, target.shape[1]-1):\n",
    "            decoding_input = tf.expand_dims(target[:, t], 1)\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(target[:, t+1], predictions)\n",
    "    batch_loss = loss / int(target.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "soviet-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 16, 256)\n",
      "(64, 16)\n",
      "(64, 16, 256)\n",
      "Epoch 1 Batch 0 Loss 0.7662\n",
      "Epoch 1 Batch 100 Loss 0.3795\n",
      "Epoch 1 Batch 200 Loss 0.3122\n",
      "Epoch 1 Batch 300 Loss 0.3042\n",
      "Epoch 1 Batch 400 Loss 0.2550\n",
      "Epoch 1 Loss 0.3281\n",
      "Time take for 1 epoch 216.32872986793518 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2325\n",
      "Epoch 2 Batch 100 Loss 0.2255\n",
      "Epoch 2 Batch 200 Loss 0.2121\n",
      "Epoch 2 Batch 300 Loss 0.1853\n",
      "Epoch 2 Batch 400 Loss 0.1512\n",
      "Epoch 2 Loss 0.1992\n",
      "Time take for 1 epoch 206.54196286201477 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1308\n",
      "Epoch 3 Batch 100 Loss 0.1470\n",
      "Epoch 3 Batch 200 Loss 0.1154\n",
      "Epoch 3 Batch 300 Loss 0.1246\n",
      "Epoch 3 Batch 400 Loss 0.0756\n",
      "Epoch 3 Loss 0.1211\n",
      "Time take for 1 epoch 206.98694396018982 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0711\n",
      "Epoch 4 Batch 100 Loss 0.0804\n",
      "Epoch 4 Batch 200 Loss 0.0919\n",
      "Epoch 4 Batch 300 Loss 0.0847\n",
      "Epoch 4 Batch 400 Loss 0.0497\n",
      "Epoch 4 Loss 0.0742\n",
      "Time take for 1 epoch 206.9731936454773 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0409\n",
      "Epoch 5 Batch 100 Loss 0.0440\n",
      "Epoch 5 Batch 200 Loss 0.0559\n",
      "Epoch 5 Batch 300 Loss 0.0523\n",
      "Epoch 5 Batch 400 Loss 0.0379\n",
      "Epoch 5 Loss 0.0471\n",
      "Time take for 1 epoch 207.65222215652466 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0329\n",
      "Epoch 6 Batch 100 Loss 0.0262\n",
      "Epoch 6 Batch 200 Loss 0.0298\n",
      "Epoch 6 Batch 300 Loss 0.0359\n",
      "Epoch 6 Batch 400 Loss 0.0154\n",
      "Epoch 6 Loss 0.0311\n",
      "Time take for 1 epoch 207.43452072143555 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0283\n",
      "Epoch 7 Batch 100 Loss 0.0203\n",
      "Epoch 7 Batch 200 Loss 0.0214\n",
      "Epoch 7 Batch 300 Loss 0.0196\n",
      "Epoch 7 Batch 400 Loss 0.0219\n",
      "Epoch 7 Loss 0.0224\n",
      "Time take for 1 epoch 208.44360184669495 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0173\n",
      "Epoch 8 Batch 100 Loss 0.0170\n",
      "Epoch 8 Batch 200 Loss 0.0196\n",
      "Epoch 8 Batch 300 Loss 0.0165\n",
      "Epoch 8 Batch 400 Loss 0.0109\n",
      "Epoch 8 Loss 0.0170\n",
      "Time take for 1 epoch 208.09531712532043 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0069\n",
      "Epoch 9 Batch 100 Loss 0.0100\n",
      "Epoch 9 Batch 200 Loss 0.0172\n",
      "Epoch 9 Batch 300 Loss 0.0132\n",
      "Epoch 9 Batch 400 Loss 0.0093\n",
      "Epoch 9 Loss 0.0140\n",
      "Time take for 1 epoch 207.98452639579773 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0137\n",
      "Epoch 10 Batch 100 Loss 0.0073\n",
      "Epoch 10 Batch 200 Loss 0.0109\n",
      "Epoch 10 Batch 300 Loss 0.0147\n",
      "Epoch 10 Batch 400 Loss 0.0086\n",
      "Epoch 10 Loss 0.0125\n",
      "Time take for 1 epoch 208.46097230911255 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    embedding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (input, target)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input, target, embedding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n",
    "    print('Time take for 1 epoch {} sec\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "sound-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_input, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = ''\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_input, decoding_hidden, encoding_outputs)\n",
    "        # attention.shape: (batch_size, input_length, 1) (1, 16, 1)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        #predictions.shape: (batch_size, vocab_size) (1,4935)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id]+' '\n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return results, input_sentence, attention_matrix\n",
    "    \n",
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention_matrix, cmap='viridis')\n",
    "    \n",
    "    font_dict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + input_sentence, fontdict = font_dict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict = font_dict, )\n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix  = evaluate(input_sentence)\n",
    "    \n",
    "    print(\"Input: %s \" % (input_sentence))\n",
    "    print(\"Predicted translation: %s\" %(results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(' ')), :len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "voluntary-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16)\n",
      "(1, 16, 256)\n",
      "Input: <start> ¿ quien eres ? <end> \n",
      "Predicted translation: who are you ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\86188\\tensor_keras\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "c:\\users\\86188\\tensor_keras\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('¿Quién eres?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
